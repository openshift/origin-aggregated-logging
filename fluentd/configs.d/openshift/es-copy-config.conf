    <store>
      @type elasticsearch_dynamic
      host "#{ENV['ES_COPY_HOST']}"
      port "#{ENV['ES_COPY_PORT']}"
      scheme "#{ENV['ES_COPY_SCHEME']}"
      index_name project.${record['kubernetes']['namespace_name']}.${record['kubernetes']['namespace_id']}.${Time.parse(record['@timestamp']).getutc.strftime(@logstash_dateformat)}
      user "#{ENV['ES_COPY_USERNAME']}"
      password "#{ENV['ES_COPY_PASSWORD']}"

      client_key "#{ENV['ES_COPY_CLIENT_KEY']}"
      client_cert "#{ENV['ES_COPY_CLIENT_CERT']}"
      ca_file "#{ENV['ES_COPY_CA']}"

      type_name com.redhat.viaq.common

      # there is currently a bug in the es plugin + excon - cannot
      # recreate/reload connections
      reload_connections false
      reload_on_failure false
      flush_interval 5s
      max_retry_wait 300
      disable_retry_limit true
      buffer_type file
      buffer_path '/var/lib/fluentd/buffer-es-copy-config'
      buffer_queue_limit "#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }"
      buffer_chunk_limit "#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }"
      # the systemd journald 0.0.8 input plugin will just throw away records if the buffer
      # queue limit is hit - 'block' will halt further reads and keep retrying to flush the
      # buffer to the remote - default is 'exception' because in_tail handles that case
      buffer_queue_full_action "#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}"
    </store>
