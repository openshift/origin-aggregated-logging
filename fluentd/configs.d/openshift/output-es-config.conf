    <store>
      @type elasticsearch_dynamic
      host "#{ENV['ES_HOST']}"
      port "#{ENV['ES_PORT']}"
      scheme https
      index_name project.${begin record['kubernetes']['namespace_name'] rescue $log.error("record is missing kubernetes.namespace_name - record " + record.to_s) end}.${begin record['kubernetes']['namespace_id'] rescue $log.error("record is missing kubernetes.namespace_id - record " + record.to_s) end}.${begin Time.parse(record['@timestamp']).getutc.strftime(@logstash_dateformat) rescue $log.error("record is missing @timestamp - record " + record.to_s) end}
      user fluentd
      password changeme

      client_key "#{ENV['ES_CLIENT_KEY']}"
      client_cert "#{ENV['ES_CLIENT_CERT']}"
      ca_file "#{ENV['ES_CA']}"

      type_name com.redhat.viaq.common

      # there is currently a bug in the es plugin + excon - cannot
      # recreate/reload connections
      reload_connections false
      reload_on_failure false
      flush_interval 5s
      max_retry_wait 300
      disable_retry_limit true
      buffer_queue_limit "#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }"
      buffer_chunk_limit "#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }"
      # the systemd journald 0.0.8 input plugin will just throw away records if the buffer
      # queue limit is hit - 'block' will halt further reads and keep retrying to flush the
      # buffer to the remote - default is 'exception' because in_tail handles that case
      buffer_queue_full_action "#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}"
    </store>
